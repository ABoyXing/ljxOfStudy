# 线性回归

线性回归是一种统计学方法，用于建立一个或多个自变量（解释变量）与一个因变量（响应变量）之间的线性关系模型。

## 最小二乘法

最小二乘法是一种数学优化技术，它通过最小化误差的平方和来寻找数据的最佳函数匹配。在线性回归中，最小二乘法用于估计线性模型的参数，使得预测值与实际观测值之间的平方误差之和最小。

公式表示为：

$$
J(\theta) = \frac{1}{2m} \sum_{i=1}^{m} (h_\theta(x^{(i)}) - y^{(i)})^2
$$

其中，$h_\theta(x^{(i)})$ 是线性回归模型的预测值，$y^{(i)}$ 是实际观测值，$m$ 是样本数，$\theta$ 是回归系数（模型参数）。

## 正规方程

正规方程是最小二乘法的一个特例，它提供了一种解析解，即不需要通过迭代算法，而是直接通过数学公式计算出最小二乘估计的参数值。

正规方程的解可以表示为：

$$
\theta = (X^T X)^{-1} X^T y
$$

其中，$X$ 是包含所有样本的特征矩阵（包括一列常数项1），$y$ 是输出向量，$\theta$ 是回归系数。

## 梯度下降法

梯度下降法是一种常用的优化算法，用于最小化一个可微分函数，通常用于机器学习中的参数优化问题。其基本思想是：通过迭代地更新参数，沿着目标函数（通常是损失函数）梯度下降的方向进行搜索，直到找到一个局部最小值。

梯度下降的更新公式为：

$$
\theta_j := \theta_j - \alpha \frac{\partial}{\partial \theta_j} J(\theta)
$$

其中，$\alpha$ 是学习率，$J(\theta)$ 是损失函数，$\frac{\partial}{\partial \theta_j} J(\theta)$ 是损失函数对参数 $\theta_j$ 的偏导数。

### 梯度下降的损失函数（MSE）

损失函数 $J(\theta)$（均方误差）为：

$$
J(\theta) = \frac{1}{2m} \sum_{i=1}^{m} (h_\theta(x^{(i)}) - y^{(i)})^2
$$

这个公式与最小二乘法中的目标函数相同，目的是最小化预测值与实际值之间的差异。